#机器学习--一些问题/思考/讨论
##介绍

--------------------------------
* 一般在分类算法中都会给出分类精度作为衡量模型好坏的标准，但在实际项目中我们却几乎不
看这个指标。为什么？因为那不是我们关注的目标。
用户有时真的很关心分类的精度吗？

* “让数据说话”没有错，关键是还要记得另一句话：兼听则明，偏听则暗！如果数据＋工具就
可以解决问题的话，还要人做什么呢？

* model的准确率和样本标注的准确率关系密切。它代表了我们model识别的上界。所以在train之前，先看看我们的上限还是很有帮助的。

* 你通过A,B,C,...方式去get 数据，反过头来若是再把A等作为识别的一列特征，那么必然导致A的权重变大，准确率变得很高。

* 避免过拟合现象的基本方法：正则化方法，regularization。正则化方法你可以理解为，对于选择函数空间的做了一种限制。使得这个函数空间比原来的函数空间小，所以不会把过分拟合的函数选择进入需要的函数空间。加入正则化之后等价于对于一些函数空间的限制

* 交叉验证只是模型选择的一种方法，如果你有模型选择问题，你就可以用交叉验证。例如你做线性回归，你有10个变量，你就有1024个模型需要选择，你就可以使用交叉验证或者AIC(最小信息准则)，做任何一件事情，都会从不同的目的去做，使用交叉验证是从预测的角度去做，使用AIC是从模型的复杂度不模型的拟合角度去做。使用p-value使用假设检验的角度去做，模型选择都是选择方法。

* 最小二乘数学建模等价于高斯噪声最大释然估计统计建模，正则化最小二乘等价于基于高斯噪声的最大化后验概率统计建模。 这里就像我说的，几乎所有的机器学习斱法也许建立之初没有什么统计解释，最后大家发现，都可以通过统计的原理解释 。

* 最小二乘用的是squre loss；svm是hinge loss。

* 数据少时用最大似然方法估计参数会过拟合，而贝叶斯方法认为模型参数有一个先验分布，因此共轭分布在贝叶斯方法中很重要。？

* 后验分布式先验和数据共同作用的结果。随着数据不断增加，参数后验分布的不确定性逐渐减少，朝一个点坍缩。

* 为什么要用Polynomial Fitting(多项式拟合)?有数学依据么，这里牵扯到范函的问题，就是函数所张成的空间，丼一个简单的例子，大家还都记得talyor展式吧：<br>
![talyor展开式](/images/jiqixuexi/ML_talyor.png)<br>
这表明 任意一个函数可以表示成 x的次方之和，也就是任意一个函数可以放到(1,x,x^2,x^3...)
所张成的函数空间，如果是有限个基的话就称为欧式空间。

* 线性model怎么构造,是单层的linear model,还是多层的linear model一直争论不休，BP否定了perceptron 的model，SVM 否定了BP model现在deep learning 又质疑SVM的shallow model，或许这就是machine learning还能前进的动力。？

* 引入基函数后,Linear regression的模型可以表达非线性的东西了，因为基函数可能是非线性的。

* LDA可以看成一个有监督降维的东西，这些PCA(主成分分析),ICA(独立成分分析)也是降低维的，不过是无监督的东西，包括mainfold dimension reduction的，都是无监督的。LDA是降低到一个投影方向上,使得它的可分性最好 而PCA是要找它的主要成分也就是使得Loss最小的方向，LDA要求类间散度最大,类内聚合度最强。类间散度最大是通过它们的均值距离体现的，而类内聚合度最强是通过类内的点到均值的散的程度表达的。如果两个类的mean完全相等,那么LDA肯定是失效的。

* (NN相关)激活函数根据实际应用确定，经常选择sigmoid函数。如果是sigmoid函数，这个神经元的输入-输出的映射就是一个logistic回归问题。这个从隐藏层到输出层的激活函数σ,根据不同应用，有不同的选择，例如回归问题的激活函数是identity函数,既分类问题的激活函数选择sigmoid函数，multiclass分类选择是softmax函数。我理解隐藏层的目的就是学习特征。卷积网络可以大大减少参数w的个数。

* 线性参数模型是通过非线性的基函数的线性组合来表达非线性的东西，模型还是线性的。

* （先验/后验）事情还没有发生,求这件事情发生的可能性的大小,是先验概率。事情已经发生,求这件事情发生的原因是由某个因素引起的可能性的大小,是后验概率。先验概率的计算比较简单，没有使用贝叶斯公式；而后验概率的计算，要使用贝叶斯公式。

* 最大似然法（Maximum Likelihood，ML）也叫极大似然估计，是一种具有理论性的点估计法。此方法的基本思想是：当从模型总体随机抽取n组样本观测值后，最合理的参数估计量应该使得从模型中抽取该n组样本观测值的概率最大，而不是像最小二乘估计法旨在得到使得模型能最好地拟合样本数据的参数估计量。

* 最小二乘法（又称最小平方法）是一种数学优化技术。它通过最小化误差的平方和寻找数据的最佳函数匹配。利用最小二乘法可以简便地求得未知的数据，并使得这些求得的数据与实际数据之间误差的平方和为最小。最小二乘法还可用于曲线拟合。其他一些优化问题也可通过最小化能量或最大化熵用最小二乘法来表达。


* 算术平均数易受极端数据的影响,这是因为平均数反应灵敏,每个数据的或大或小的变化都会影响到最终结果。有时，我们需要用中位数来代替它。

* 数学调和平均数定义为：数值倒数的平均数的倒数。
调和平均数具有以下几个主要特点：
1 调和平均数易受极端值的影响，且受极小值的影响比受极大值的影响更大。
2 只要有一个标志值为0，就不能计算调和平均数。（可以通过加上一个值很小的b来防止分母为0的情况。）
3 调和平均数应用的范围较小。在实际中，往往由于缺乏总体单位数的资料而不能直接计算算术平均数，这时需用调和平均法来求得平均数。

* 几何平均数，为n个观察值连乘积的n次方根。样本数据非负,主要用于对数正态分布。

* 算术平均数、调和平均数、几何平均数是三种不同形式的平均数，分别有各自的应用条件。进行统计研究时，适宜采用算术平均数时就不能用调和平均数或几何平均数，适宜用调和平均数时，同样也不能采用其他两种平均数。
计算以上三种平均数的结果是：算术平均数大于几何平均数，而几何平均数又大于调和平均数。当所有的变量值都相等时，则这三种平均数就相等。

* 有时需要将一些特征转换为一个得分，那么在设计这个得分函数的时候，要考虑趋势，正负相关，线性/非线性相关，以及在两端时的影响。综合来设计，虽然不唯一，但要体现上述信息又要做到平滑。

* 策略调研五问：
```
1 你要最终的目的要做什么？
2 你打算怎么做？
3 你打算做完之后，怎么评估？
4 为什么这做是有效的？
5 你是怎么样保证这个过程的正确性？
```
* 回归和分类最大的区别在于模型产出的数值之间的可比性。

* Bag of words，也叫做“词袋”，在信息检索中，Bag of words model假定对于一个文本，忽略其词序和语法，句法，将其仅仅看做是一个词集合，或者说是词的一个组合，文本中每个词的出现都是独立的，不依赖于其他词是否出现，或者说当这篇文章的作者在任意一个位置选择一个词汇都不受前面句子的影响而独立选择的。

* 选择一个合理的算法可以从很多方面来考察，包括：
```
训练实例的数量？
特征空间的维度？
是否希望该问题线性可分？
特征是否是独立的？
是否预期特征能够线性扩展？
过度拟合是否会成为一个问题？
系统在速度/性能/内存使用等方面的要求如何？
```

* LR除了对简单高效，以及对噪声相当强劲外，另一个优点是输出可以被解释为概率。

* SVM代替LR的原因可能是线性不可分或是高维空间。

* RF和GBDT之间的差别，可以简单理解为GBDT的性能通常会更好，但它们更难保证正确。更具体而言，GBDT有更多的超参数需要调整，并且也更容易出现过度拟合。RF几乎可以“开箱即用”。


--------------------------------
######（转载本站文章请注明作者和出处 <a href="https://github.com/MangoLiu">MangoLiu</a> ，请勿用于任何商业用途）

