#机器学习_决策树
##介绍
--------------------------------
举个简单的例子，先有个感性的认识：他人给介绍相亲时，总会问几个简单的问题，然后判断是否有见面的必要。这个过程实际上就是一个决策树。我们使用决策树来处理分类问题，它包含判断模块，终止模块。先看一个简单的决策树表示图：

![决策树举例](/images/jiqixuexi/jueceshu_jueceshu.png)

决策树是根据训练数据来构造的，之后可用于分析数据，或用作预测。<br>
优点：复杂度不高，便于理解。<br>
缺点：可能会产生过度匹配问题。<br>
难点：<br>
>   1 如果有很多特征，选取哪些特征，按照什么顺序才能更好的进行分类？<br>
    2 递归构造以及结束条件的判断。<br>
    3 过度匹配问题的处理。<br>
    4 通过训练集构造决策树，如何将其保存？<br>

有兴趣可以先看看关于决策树的介绍：[决策树学习(wikipedia)](http://zh.wikipedia.org/wiki/%E5%86%B3%E7%AD%96%E6%A0%91%E5%AD%A6%E4%B9%A0)

## 划分数据集
--------------------------------
在学习如何选取最好的特征来划分数据之前，我们先来学习下<strong>熵</strong>(entropy)的概念。熵是集合信息的一种度量方式，可以表示划分数据集的前后信息发生的变化。它的公式为：<br>
>![熵](/images/jiqixuexi/jueceshu_shang.png)

假设分类后，有C种结果，每种的概率为p(x)，结果切分的越均匀，entropy的值越大。所以，我们的目标是计算每一种特征的熵，选择最大值的特征，作为当前分类的判断节点。
关于熵的介绍：[熵(wikipedia)](http://zh.wikipedia.org/wiki/%E4%BF%A1%E6%81%AF%E7%86%B5)


另一种度量集合无序程度的方法是基尼不纯度，简单地说就是从一个数据集中随机选取子项，度量其被错误分类到其他分组的概率。基尼不纯度可以通过如下计算：
>![基尼不纯度](/images/jiqixuexi/jueceshu_jini.PNG) <br>

然后。对每个特征划分一次数据集，计算数据集的新熵值，之后比较并返回最好特征划分的索引值。

## 利用信息增益构建决策树举例
----------------------------

**信息增益**是特征选择中的一个重要指标，它定义为一个特征能够为分类系统带来多少信息，带来的信息越多，该特征越重要。那么如何衡量一个特征为分类系统带来的信息多少呢？对一个特征而言，系统有它和没它时信息量将发生变化，而前后信息量的差值就是这个特征给系统带来的信息量。所谓信息量，其实就是熵。<br>

信息增益和熵的关系:信息增益是针对一个一个的特征而言的，就是看一个特征t，系统有它和没它的时候信息量各是多少，两者的差值就是这个特征给系统带来的信息量，即增益。系统含有特征t的时候信息量很好计算，就是刚才的式子，它表示的是包含所有特征时系统的信息量。<br>

问题是当系统不包含t时，信息量如何计算？
我们计算分类系统不包含特征t的时候，就使用下面情况来代替，就是计算当一个特征t不能变化时，系统的信息量是多少。这个信息量其实也有专门的名称，就叫做“条件熵”，条件嘛，自然就是指“t已经固定“这个条件。<br>

但是问题接踵而至，例如一个特征X，它可能的取值有n多种（x1，x2，……，xn）， 当计算条件熵而需要把它固定的时候，要把它固定在哪一个值上呢？答案是每一种可能都要固定一下，计算n个值，然后取均值才是条件熵。而取均值也不是简单的 加一加然后除以n，而是要用每个值出现的概率来算平均（简单理解，就是一个值出现的可能性比较大，固定在它上面时算出来的信息量占的比重就要多一些）。<br>

因此特征T给系统带来的信息增益就可以写成系统原本的熵与固定特征T后的条件熵之差：<br>
>![IG](/images/jiqixuexi/jueceshu_ig.PNG) <br>

这个数据集来自Mitchell的机器学习，叫做是否去打网球play-tennis。
其中6个变量依次为：编号、天气{Sunny、Overcast、Rain}、温度{热、冷、适中}、
湿度{高、正常}、风力{强、弱}以及最后是否去玩的决策{是、否}。
```
NO. , Outlook , Temperature , Humidity , Wind , Play 
1 , Sunny , Hot , High , Weak , No 
2 , Sunny , Hot , High , Strong , No 
3 , Overcast , Hot , High , Weak , Yes 
4 , Rain , Mild , High , Weak , Yes 
5 , Rain , Cool , Normal , Weak , Yes 
6 , Rain , Cool , Normal , Strong , No 
7 , Overcast , Cool , Normal , Strong , Yes 
8 , Sunny , Mild , High , Weak , No 
9 , Sunny , Cool , Normal , Weak , Yes 
10 , Rain , Mild , Normal , Weak , Yes 
11 , Sunny , Mild , Normal , Strong , Yes 
12 , Overcast , Mild , High , Strong , Yes 
13 , Overcast , Hot , Normal , Weak , Yes 
14 , Rain , Mild , High , Strong , No
```

1 计算分类系统熵.
类别是 是否出去玩。取值为yes的记录有9个，取值为no的有5个，即说这个样本里有9个正例，5个负例，
记为S(9+,5-)，S是样本的意思(Sample)。那么P(c1) = 9/14, P(c2) = 5/14
这里熵记为Entropy(S),计算公式为：
Entropy(S)= -(9/14)*log2(9/14)-(5/14)*log2(5/14)。

2 分别以Wind、Humidity、Outlook和Temperature作为根节点，计算其信息增益.
我们来计算Wind的信息增益
当Wind固定为Weak时：记录有8条，其中正例6个，负例2个；
同样，取值为Strong的记录6个，正例负例个3个。我们可以计算相应的熵为：
Entropy(Weak)=-(6/8)*log(6/8)-(2/8)*log(2/8)=0.811
Entropy(Strong)=-(3/6)*log(3/6)-(3/6)*log(3/6)=1.0

3 现在就可以计算出相应的信息增益了：
所以，对于一个Wind属性固定的分类系统的信息量为 (8/14)*Entropy(Weak)+(6/14)*Entropy(Strong)
Gain(Wind)=Entropy(S)-(8/14)*Entropy(Weak)-(6/14)*Entropy(Strong)=0.940-(8/14)*0.811-(6/14)*1.0=0.048
这个公式的奥秘在于，8/14是属性Wind取值为Weak的个数占总记录的比例，
同样6/14是其取值为Strong的记录个数与总记录数之比。

4 同理，如果以Humidity作为根节点：
Entropy(High)=0.985 ; Entropy(Normal)=0.592
Gain(Humidity)=0.940-(7/14)*Entropy(High)-(7/14)*Entropy(Normal)=0.151
以Outlook作为根节点：
Entropy(Sunny)=0.971 ; Entropy(Overcast)=0.0 ; Entropy(Rain)=0.971
Gain(Outlook)=0.940-(5/14)*Entropy(Sunny)-(4/14)*Entropy(Overcast)-(5/14)*Entropy(Rain)=0.247
以Temperature作为根节点：
Entropy(Cool)=0.811 ; Entropy(Hot)=1.0 ; Entropy(Mild)=0.918
Gain(Temperature)=0.940-(4/14)*Entropy(Cool)-(4/14)*Entropy(Hot)-(6/14)*Entropy(Mild)=0.029
这样我们就得到了以上四个属性相应的信息增益值：
Gain(Wind)=0.048 ；Gain(Humidity)=0.151 ； Gain(Outlook)=0.247 ；Gain(Temperature)=0.029

最后按照信息增益最大的原则选Outlook为根节点。子节点重复上面的步骤。
这颗树可以是这样的，它读起来就跟你认为的那样：

>![demo](/images/jiqixuexi/jueceshu_demo.PNG) 


## 递归构建决策树
--------------------------------
划分一次后，数据将向下传递到树分支的下个节点，在这个节点上，我们可以再次划分数据，因此我们可以采用递归的原则处理数据集。但递归总是要有结束条件的，这里的结束条件是什么呢？<br>

递归结束的条件是：程序遍历完所有划分数据集的属性，或者每个分支下的所有实例都具有相同的分类。若得到的关键字的值是类标签，则表示其是一个叶子节点，否则是一个判断节点，继续构造。<br>

但是假如数据集已经处理了所有的属性，但是类标签依然不是唯一的。怎么办？我们通常会采用多数表决非方法决定该叶子节点的分类。<br>

构造决策树的目的在于后续的使用，若是每次都根据训练集来生成，则会消耗很多时间。因此最好将生成好的决策树进行序列化操作，保存在硬盘上，并在需要的时候读取出来。（任何对象都可以执行序列化操作，决策树也不例外）<br>


## 总结
--------------------------------
假如给定一些包含各种特征的数据，我们应该这样做：

1.假如当前的数据都属于同一类，则结束计算划分，并添加终止模块，其中记录当前的类标识；<br>
2.若是所有的属性都已消耗完，剩余的数据不属于同一类别，则按照多数表决的方式决定；<br>
3.遍历每个特征来划分数据（每划分一次，消耗一个特征），对每个结果集求熵，选取最大的作为当前划分特征；（若是熵有相同的情形，任选一个）<br>
4.否则，按照上述递归构建子树；<br>
5.序列化保存构建好的决策树，方便后续使用。<br>
6.找到最佳决策树是NP问题。<br>
7.决策树一旦建立，对未知的样本判定非常快。O(w)，其中w为树的深度。<br>
8.奥卡姆剃刀法则：给定两个具有相同泛化误差的模型，较简单的更加可取。<br>

--------------------------------
######（转载本站文章请注明作者和出处 <a href="https://github.com/MangoLiu">MangoLiu</a> ，请勿用于任何商业用途）

